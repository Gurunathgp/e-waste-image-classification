# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_LbRkaOVlrEVUzD_mhaDOABc2eCzmvqT
"""

from google.colab import files
uploaded = files.upload()  # Upload the zip file here

import zipfile
import os

# Extract the uploaded file
with zipfile.ZipFile("archive (2).zip", 'r') as zip_ref:
    zip_ref.extractall("e_waste_data")

# Check folders inside
os.listdir("e_waste_data")

!pip install tensorflow gradio

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.applications import EfficientNetV2B0
from tensorflow.keras.applications.efficientnet import preprocess_input

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

import os

# View top-level folders inside the dataset
os.listdir("e_waste_data")

os.listdir("e_waste_data/modified-dataset")

import tensorflow as tf

img_size = (128, 128)
batch_size = 32

train_path = "e_waste_data/modified-dataset/train"
val_path = "e_waste_data/modified-dataset/val"
test_path = "e_waste_data/modified-dataset/test"

train_ds = tf.keras.utils.image_dataset_from_directory(
    train_path, image_size=img_size, batch_size=batch_size, shuffle=True)

val_ds = tf.keras.utils.image_dataset_from_directory(
    val_path, image_size=img_size, batch_size=batch_size, shuffle=True)

test_ds = tf.keras.utils.image_dataset_from_directory(
    test_path, image_size=img_size, batch_size=batch_size, shuffle=False)

base_model = EfficientNetV2B0(include_top=False, input_shape=(128, 128, 3), weights='imagenet')
base_model.trainable = False

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dropout(0.2),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(len(train_ds.class_names), activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(train_ds, validation_data=val_ds, epochs=10)

loss, acc = model.evaluate(test_ds)
print(f"Test Accuracy: {acc:.2f}")

y_pred = []
y_true = []

for images, labels in test_ds:
    preds = model.predict(images)
    y_pred.extend(np.argmax(preds, axis=1))
    y_true.extend(labels.numpy())

print(classification_report(y_true, y_pred, target_names=train_ds.class_names))

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', xticklabels=train_ds.class_names, yticklabels=train_ds.class_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()